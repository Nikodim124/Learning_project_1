<!doctype html>
<html>
  <head>
      <meta http-equiv="Content-type" content="text/html; charset=utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=Edge">
      <title>ПРОЕКТ ПО УЧЕБНОЙ ПРАКТИКЕ</title>
      <link rel="stylesheet" href="css/styles.css" type="text/css">
      <link rel="preconnect" href="https://fonts.gstatic.com">
      <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,700;1,300&display=swap" rel="stylesheet">
  </head>
    <body>
    <div class="sidebar">
        <header><a href="https://pspu.ru"><img src="images/Group.png" alt="ПГГПУ" height="150px"></a></header>
        <ul>
          <li><a href="index.htm">Главная</a></li>
          <li><a class="current" href="teor.htm">Теория</a></li>
          <ul>
            <li><a class="sub" href="teor1.htm">Теоретическая часть 1</a></li>
            <li><a class="sub" href="teor2.htm">Теоретическая часть 2</a></li>
            <li><a class="sub" href="teor3.htm">Теоретическая часть 3</a></li>
          </ul>
          <li><a href="prac.htm">Практика</a></li>
          <li><a href="avt.htm">Авторы</a></li>
        </ul>
      </div>
      <div class="content">
        <div class="head">ПРОЕКТ ПО УЧЕБНОЙ ПРАКТИКЕ</div>
        <div class="text">
          <p>По большому счёту, обучение строится на снижении неопределённости данных. Что вообще можно подразумевать под неопределённостью, когда речь идёт о задаче классификации? Здесь нужно ввести довольно важный термин для всех темы связанной с обучением - это <b>энтропия</b>.</p>
          <p>По большому счёту, энтропия - это уровень неопределённости наших данных. Математическим языком это можно выразить так:</p>
          <div class="formula">
          	<img src="images/formula.svg">
          </div>
          <p>Эта формула называется формулой Шеннона. Изначально в нашем случае, как зачастую и бывает, наши данные максимально неопределённые, то есть <b>p = 0,5</b>, а значит вся энтропия равна <b>1</b>. Это значит, что любая попытка предсказать без наших параметров привела бы нас к распределению <b>50 на 50</b>, то есть распределние было бы абсолютно случайно.</p>
          <p>Для того, чтобы избежать такого подхода, мы и нашли и отобрали те параметры, которые наша модель будет учитывать. Иными словами, мы даём нашей модели те параметры, которые она будет учитывать при прогнозировании, и от которых она будет отталкиваться. По сути, мы пробегаемся по всем значениям нашего выбранного параметра и пытаемся сместить центр распределения с 50% в какую-либо сторону. Очень хорошо, если энтропия упадёт до 0, но при условии достаточно большого количества наблюдений. Это будет означать, что данный параметр точно определяет искомый нами класс.</p>
          <p>После этого мы можем посчитать, насколько более упорядоченной становится для нас искомая переменная Y, если мы знаем значения параметра X. Или, говоря проще, существует ли корреляция между значениями X и Y, и насколько она велика. Об этом говорит величина <b>information gain</b>:</p>
          <div class="formula">
          	<img src="images/ig.svg">
          </div>
          <p>Чем больше параметр information gain — тем сильнее корреляция. Таким образом, мы легко можем вычислить information gain для всех параметров и выкинуть те, которые слабо влияют на целевую переменную. Тем самым, во-первых, сократив время расчета модели, а, во-вторых, уменьшив опасность переобучения.</p>
          <p>Пользуясь этими формулами мы найдём такие параметры, которые помогут нам предсказывать искомый класс на основании предскавленных параметров.</p>
        </div>
        <form action="teor1.htm" class="last">
          <button class="next">Далее</button>
        </form>
      </div>
  </body>
</html>